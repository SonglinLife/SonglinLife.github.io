<!-- build time:Wed Dec 22 2021 20:44:44 GMT+0800 (China Standard Time) --><!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#FFF"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"><link rel="icon" type="image/ico" sizes="32x32" href="/images/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="alternate" type="application/rss+xml" title="Life" href="https://songlinlife.top/rss.xml"><link rel="alternate" type="application/atom+xml" title="Life" href="https://songlinlife.top/atom.xml"><link rel="alternate" type="application/json" title="Life" href="https://songlinlife.top/feed.json"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CFredericka%20the%20Great:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20JP:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CInconsolata:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/css/app.css?v=0.2.5"><meta name="keywords" content="GNN,HAN"><link rel="canonical" href="https://songlinlife.top/2021/GNN-02-HAN-Heterogeneous-Graph-Attention-Network-%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/"><title>GNN(02): HAN(Heterogeneous Graph Attention Network)代码复现 - AI | Songlin = Life</title><meta name="generator" content="Hexo 5.4.0"></head><body itemscope itemtype="http://schema.org/WebPage"><div id="loading"><div class="cat"><div class="body"></div><div class="head"><div class="face"></div></div><div class="foot"><div class="tummy-end"></div><div class="bottom"></div><div class="legs left"></div><div class="legs right"></div></div><div class="paw"><div class="hands left"></div><div class="hands right"></div></div></div></div><div id="container"><header id="header" itemscope itemtype="http://schema.org/WPHeader"><div class="inner"><div id="brand"><div class="pjax"><h1 itemprop="name headline">GNN(02): HAN(Heterogeneous Graph Attention Network)代码复现</h1><div class="meta"><span class="item" title="创建时间：2021-12-18 10:54:08"><span class="icon"><i class="ic i-calendar"></i> </span><span class="text">发表于</span> <time itemprop="dateCreated datePublished" datetime="2021-12-18T10:54:08+08:00">2021-12-18</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span class="text">本文字数</span> <span>2.4k</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span class="text">阅读时长</span> <span>2 分钟</span></span></div></div></div><nav id="nav"><div class="inner"><div class="toggle"><div class="lines" aria-label="切换导航栏"><span class="line"></span> <span class="line"></span> <span class="line"></span></div></div><ul class="menu"><li class="item title"><a href="/" rel="start">Songlin</a></li></ul><ul class="right"><li class="item theme"><i class="ic i-sun"></i></li><li class="item search"><i class="ic i-search"></i></li></ul></div></nav></div><div id="imgs" class="pjax"><ul><li class="item" data-background-image="https://tva2.sinaimg.cn/large/6833939bly1gipewkhf1zj20zk0m81kx.jpg"></li><li class="item" data-background-image="https://tva2.sinaimg.cn/large/6833939bly1gicitzannuj20zk0m8b29.jpg"></li><li class="item" data-background-image="https://tva2.sinaimg.cn/large/6833939bly1gicliwyw55j20zk0m8hdt.jpg"></li><li class="item" data-background-image="https://tva2.sinaimg.cn/large/6833939bly1giclj61ylzj20zk0m8b29.jpg"></li><li class="item" data-background-image="https://tva2.sinaimg.cn/large/6833939bly1giclx29mstj20zk0m8hdt.jpg"></li><li class="item" data-background-image="https://tva2.sinaimg.cn/large/6833939bly1gipetlbztpj20zk0m84qp.jpg"></li></ul></div></header><div id="waves"><svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"/></defs><g class="parallax"><use xlink:href="#gentle-wave" x="48" y="0"/><use xlink:href="#gentle-wave" x="48" y="3"/><use xlink:href="#gentle-wave" x="48" y="5"/><use xlink:href="#gentle-wave" x="48" y="7"/></g></svg></div><main><div class="inner"><div id="main" class="pjax"><div class="article wrap"><div class="breadcrumb" itemscope itemtype="https://schema.org/BreadcrumbList"><i class="ic i-home"></i> <span><a href="/">首页</a></span><i class="ic i-angle-right"></i> <span class="current" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/ai/" itemprop="item" rel="index" title="分类于 AI"><span itemprop="name">AI</span></a><meta itemprop="position" content="1"></span></div><article itemscope itemtype="http://schema.org/Article" class="post block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://songlinlife.top/2021/GNN-02-HAN-Heterogeneous-Graph-Attention-Network-%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="Kalice"><meta itemprop="description" content=", Life is not about lifestyle, it means Lithium and Ferrum."></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Life"></span><div class="body md" itemprop="articleBody"><h1 id="han算法复现"><a class="anchor" href="#han算法复现">#</a> HAN 算法复现</h1><h3 id="异构图"><a class="anchor" href="#异构图">#</a> 异构图</h3><p>异构图可以表示为图<img data-src="https://image-2021-wu.oss-cn-beijing.aliyuncs.com/blogs/picturesclip_image002.png" alt="img">，有节点类型映射<img data-src="https://image-2021-wu.oss-cn-beijing.aliyuncs.com/blogs/picturesclip_image004.png" alt="img">，边类型映射<img data-src="https://image-2021-wu.oss-cn-beijing.aliyuncs.com/blogs/picturesclip_image006.png" alt="img">。其中 V 是节点集合，E 是边集合，A 是节点类型集合，R 是关系型集合，有<img data-src="https://image-2021-wu.oss-cn-beijing.aliyuncs.com/blogs/picturesclip_image008.png" alt="img"> 成立。</p><p>​ 因为异构图的节点和边类型不一致，一个常用的方法就是引入元路径（meta-path），将异构图转换为同构图。 <img data-src="https://image-2021-wu.oss-cn-beijing.aliyuncs.com/blogs/picturesclip_image010.png" alt="img">。其中<img data-src="https://image-2021-wu.oss-cn-beijing.aliyuncs.com/blogs/picturesclip_image012.png" alt="img"> 代表了路径的长度。通过元路径就可以对相邻节点进行分类，进而将异构图转换为一组同构图，因此异构图也被称为多通道网络。</p><p>​ 例如，如图 2.4 所示，IMDB 图是一个典型的异构图，有三种节点类型，演员、电影、导员。可以定义两个元路径，电影 - 演员 - 电影（MAM）和电影 - 导演 - 电影（MDM）。通过元路径就可以就可以将该网络转换为基于不同元路径的电影网络。</p><p><img data-src="https://image-2021-wu.oss-cn-beijing.aliyuncs.com/blogs/picturesclip_image014.jpg" alt="img"></p><h3 id="节点级别的attention"><a class="anchor" href="#节点级别的attention">#</a> 节点级别的 attention</h3><p>AN 模型可以划分为两个主要的部分，一个节点级别的 Attention 另一个是语义级别的 Attention。</p><p>​ 节点级别的 Attention 和 GAT 的原理相一致。首先 HAN 会根据定义的元路径生成元路径个数的同构图，在这些同构图上使用 GAT 算法，就是是节点级别的 Attention。但值得注意的是，因为异质图的节点特征维度可能不一致，在论文中是采用了映射方法，将所有不同类型的节点的特征映射到同一向量空间。</p><p>​ 由于基于相同元路径<img data-src="https://image-2021-wu.oss-cn-beijing.aliyuncs.com/blogs/picturesclip_image002.png" alt="img"> 的节点邻居的类型都是一致的，所以可以直接采用 GAT 计算 Attention 系数的方法，但这里在经过 softmax 层之前还使用了激活函数：</p><p>​ <img data-src="https://image-2021-wu.oss-cn-beijing.aliyuncs.com/blogs/picturesclip_image004.png" alt="img"> （2.9）</p><p>​ 于是，节点 i 基于元路径<img data-src="https://image-2021-wu.oss-cn-beijing.aliyuncs.com/blogs/picturesclip_image002.png" alt="img"> 的 embedding 向量可以有临域节点的向量聚合得到：</p><p>​ <img data-src="https://image-2021-wu.oss-cn-beijing.aliyuncs.com/blogs/picturesclip_image007.png" alt="img"> （2.10）</p><p>​ 同理，如果使用了多头 Attention 机制，需要将所有输入的 embedding 结果拼接起来：</p><p>​ <img data-src="https://image-2021-wu.oss-cn-beijing.aliyuncs.com/blogs/picturesclip_image009.png" alt="img"> （2.11）</p><p>​ <img data-src="https://image-2021-wu.oss-cn-beijing.aliyuncs.com/blogs/picturesclip_image011.png" alt="img"> 就是特定元路径下的 embedding 向量，在给定 P 条元路径<img data-src="https://image-2021-wu.oss-cn-beijing.aliyuncs.com/blogs/picturesclip_image013.png" alt="img"> 下，经过节点级别的 Attention 之后，会得到 P 个特定元路径下节点的 embedding，可以表示为<img data-src="https://image-2021-wu.oss-cn-beijing.aliyuncs.com/blogs/picturesclip_image015.png" alt="img">。</p><h3 id="语义级别的attention"><a class="anchor" href="#语义级别的attention">#</a> 语义级别的 attention</h3><p>​ 通常，每个节点在异质图中都会包含多种语言的信息，特定语义下的 node embedding 只能反映节点某个方面的性质。为了学习到更全面的 node embedding，需要将多个元路径下的语义混合起来。HAN 使用了通过一个语义级别的 Attention 可以自动学习不同元路径的中重要性。假设要 P 条元路径，就有 P 个特定语义下的 node embedding，学习到的不同元路径权重可以表示为：</p><p>​ <img data-src="https://image-2021-wu.oss-cn-beijing.aliyuncs.com/blogs/picturesclip_image002.png" alt="img"> （2.12)</p><p>​ HAN 算法首先将输入的向量<img data-src="https://image-2021-wu.oss-cn-beijing.aliyuncs.com/blogs/picturesclip_image004.png" alt="img"> 经过一层全连接层，再和语义级别的 Attention 向量<img data-src="https://image-2021-wu.oss-cn-beijing.aliyuncs.com/blogs/picturesclip_image006.png" alt="img"> 做点积，并且将该元路径下的所有节点<img data-src="https://image-2021-wu.oss-cn-beijing.aliyuncs.com/blogs/picturesclip_image008.png" alt="img"> 得到的值做平均，得到最终的重要系数<img data-src="https://image-2021-wu.oss-cn-beijing.aliyuncs.com/blogs/picturesclip_image010.png" alt="img">：</p><p>​ <img data-src="https://image-2021-wu.oss-cn-beijing.aliyuncs.com/blogs/picturesclip_image012.png" alt="img"> （2.13）</p><p>​ 再通过一层 softmax 归一化层，输出语义级别的 Attention 值：</p><p>​ <img data-src="https://image-2021-wu.oss-cn-beijing.aliyuncs.com/blogs/picturesclip_image014.png" alt="img"> （2.14）</p><p>​ 最终，混合所有的语义信息得到整个 node embedding 矩阵<img data-src="https://image-2021-wu.oss-cn-beijing.aliyuncs.com/blogs/picturesclip_image016.png" alt="img">：</p><p>​ <img data-src="https://image-2021-wu.oss-cn-beijing.aliyuncs.com/blogs/picturesclip_image018.png" alt="img"></p><p><img data-src="https://image-2021-wu.oss-cn-beijing.aliyuncs.com/blogs/picturesimage-20211222204146489.png" alt="image-20211222204146489"></p><h3 id="acm数据集"><a class="anchor" href="#acm数据集">#</a> ACM 数据集</h3><p>ACM：Han 算法使用了 ACM 引文数据集，作者从 KDD，SIGMOD，SIGCOMM，MobICOMM 和 VLDB 上抽取了 3025 篇 paper，这些 paper 可以可以划分到 3 个领域（Database，Wireless Communication，Data Mining）。一共包括 3025 份 papers（P），5835 个 authors（A），56 个 subjects（S），包括两条元路径 {PAP，PSP}。</p><h3 id="复现结果"><a class="anchor" href="#复现结果">#</a> 复现结果</h3><p>HAN 模型搭建完毕之后，在 ACM 数据集上做了测试。一样，我使用了对照组对模型效果进行验证。对于 GCN、GAT 这两个网络由于它们是同构图网络模型，因此无法直接运用在异构图上。采取的做法是，将异构图使用 metapath 进行转换，提取出两个同构图并分别进行使用 GCN 模型和 GAT 模型进行预测。</p><p>​ 因为 HAN 的 Attention 分为节点级别和语义级别，因此分别抽取了没有节点级别和没有语义级别的 Attention 网络，即 HAN_sem 和 HAN_ed。</p><p>​ 从最终的复现结果中可以看出，GAT 的效果要优于 GCN 的效果。而 HAN 的预测效果要优于 GAT 和 GCN 模型。并且，即使是只有单个级别的 Attention 的 HAN 模型，其预测效果也要好于 HAN 模型和 GCN 模型。</p><table><thead><tr><th></th><th><strong>GCN</strong></th><th><strong>GAT</strong></th><th><strong>HAN_sem</strong></th><th><strong>HAN_nd</strong></th><th><strong>HAN</strong></th></tr></thead><tbody><tr><td>Micro F1</td><td>0.8442/0.6687</td><td>0.8508/0.6664</td><td>0.8673</td><td>0.8640</td><td>0.8819</td></tr><tr><td>Macro F1</td><td>0.8430/0.66709</td><td>0.8503/0.6743</td><td>0.8673</td><td>0.8651</td><td>0.8820</td></tr></tbody></table><p>表 4.4 实验复现结果</p><h3 id="attention分析"><a class="anchor" href="#attention分析">#</a> Attention 分析</h3><p>​ HAN 模型除了预测准确度高，它的可解释性也很强。经过训练之后的模型，对于两条元路径 paper-author-paper 和 paper-subject-paper 给出不同的 Attention 值。可以看到模型认为 PAP 元路径，即同一作者下的 paper 之间的形成的 graph，它的重要性要高于 PSP 元路径。这和我们的直觉相一致，即同一作者下的 paper 之间的关联更加紧密。</p><p><img data-src="https://image-2021-wu.oss-cn-beijing.aliyuncs.com/blogs/picturesimage-20211222204300258.png" alt="image-20211222204300258"></p><h3 id="422-聚类效果"><a class="anchor" href="#422-聚类效果">#</a> 4.2.2 聚类效果</h3><p>​ 因为上述提到的模型可以用来提取 node embedding，因此我也做了聚类分析。对于 GAT 和 GCN 模型，选择最优的那条元路径子图训练得到的模型生成 embedding。对于所有模型，embedding 的维度都设置为 64。</p><p>​ 在得到了节点的 embedding 之后，使用 kmeans 无监督聚类方式进行聚类，kmeans 的 K 值设置为节点的标签种类数即 3。使用 NMI 和 ARI 两个指标进行分析，结果如表 4.5 所示。可以看到 HAN 的聚类效果要比 GCN 和 GAT 模型好不少</p><table><thead><tr><th></th><th><strong>GCN</strong></th><th><strong>GAT</strong></th><th><strong>HAN</strong></th></tr></thead><tbody><tr><td>NMI</td><td>0.5557</td><td>0.5289</td><td>0.6413</td></tr><tr><td>ARI</td><td>0.5553</td><td>0.5286</td><td>0.6411</td></tr></tbody></table><h3 id="423-可视化"><a class="anchor" href="#423-可视化">#</a> 4.2.3 可视化</h3><p>​ 为了进一步地给出比较，使用了 t-SNE 方法对于 embedding 进行降维到 2D 进行可视化分析。可以看到 HAN 算法给在 wireless Communication 的论文里，给出了两个比较分离的簇。可能是 HAN 算法能够找个更加多的信息，从而使得其效果优于 GCN 和 GAT。</p><p><img data-src="https://image-2021-wu.oss-cn-beijing.aliyuncs.com/blogs/picturesimage-20211222204328470.png" alt="image-20211222204328470"><img data-src="https://image-2021-wu.oss-cn-beijing.aliyuncs.com/blogs/picturesimage-20211222204332270.png" alt="image-20211222204332270"></p><p><img data-src="https://image-2021-wu.oss-cn-beijing.aliyuncs.com/blogs/picturesimage-20211222204336878.png" alt="image-20211222204336878"></p><div class="tags"><a href="/tags/GNN/" rel="tag"><i class="ic i-tag"></i> GNN</a> <a href="/tags/HAN/" rel="tag"><i class="ic i-tag"></i> HAN</a></div></div><footer><div class="meta"><span class="item"><span class="icon"><i class="ic i-calendar-check"></i> </span><span class="text">更新于</span> <time title="修改时间：2021-12-22 20:44:11" itemprop="dateModified" datetime="2021-12-22T20:44:11+08:00">2021-12-22</time></span></div><div class="reward"><button><i class="ic i-heartbeat"></i> 赞赏</button><p>请我喝[茶]~(￣▽￣)~*</p><div id="qr"><div><img data-src="/images/wechatpay.png" alt="Kalice 微信支付"><p>微信支付</p></div><div><img data-src="/images/alipay.png" alt="Kalice 支付宝"><p>支付宝</p></div></div></div><div id="copyright"><ul><li class="author"><strong>本文作者： </strong>Kalice <i class="ic i-at"><em>@</em></i>Life</li><li class="link"><strong>本文链接：</strong> <a href="https://songlinlife.top/2021/GNN-02-HAN-Heterogeneous-Graph-Attention-Network-%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/" title="GNN(02): HAN(Heterogeneous Graph Attention Network)代码复现">https://songlinlife.top/2021/GNN-02-HAN-Heterogeneous-Graph-Attention-Network-代码复现/</a></li><li class="license"><strong>版权声明： </strong>本站所有文章除特别声明外，均采用 <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpo"><i class="ic i-creative-commons"><em>(CC)</em></i>BY-NC-SA</span> 许可协议。转载请注明出处！</li></ul></div></footer></article></div><div class="post-nav"><div class="item left"><a href="/2021/ai/DGL-04-%E8%AE%AD%E7%BB%83%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" itemprop="url" rel="prev" data-background-image="https:&#x2F;&#x2F;tva2.sinaimg.cn&#x2F;mw690&#x2F;6833939bly1giclxfdlttj20zk0m8npd.jpg" title="DGL(04): 训练图神经网络"><span class="type">上一篇</span> <span class="category"><i class="ic i-flag"></i> AI</span><h3>DGL(04): 训练图神经网络</h3></a></div><div class="item right"></div></div><div class="wrap" id="comments"></div></div><div id="sidebar"><div class="inner"><div class="panels"><div class="inner"><div class="contents panel pjax" data-title="文章目录"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#han%E7%AE%97%E6%B3%95%E5%A4%8D%E7%8E%B0"><span class="toc-number">1.</span> <span class="toc-text">HAN 算法复现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%82%E6%9E%84%E5%9B%BE"><span class="toc-number">1.0.1.</span> <span class="toc-text">异构图</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%8A%82%E7%82%B9%E7%BA%A7%E5%88%AB%E7%9A%84attention"><span class="toc-number">1.0.2.</span> <span class="toc-text">节点级别的 attention</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%AD%E4%B9%89%E7%BA%A7%E5%88%AB%E7%9A%84attention"><span class="toc-number">1.0.3.</span> <span class="toc-text">语义级别的 attention</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#acm%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">1.0.4.</span> <span class="toc-text">ACM 数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%8D%E7%8E%B0%E7%BB%93%E6%9E%9C"><span class="toc-number">1.0.5.</span> <span class="toc-text">复现结果</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#attention%E5%88%86%E6%9E%90"><span class="toc-number">1.0.6.</span> <span class="toc-text">Attention 分析</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#422-%E8%81%9A%E7%B1%BB%E6%95%88%E6%9E%9C"><span class="toc-number">1.0.7.</span> <span class="toc-text">4.2.2 聚类效果</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#423-%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="toc-number">1.0.8.</span> <span class="toc-text">4.2.3 可视化</span></a></li></ol></li></ol></div><div class="related panel pjax" data-title="系列文章"><ul><li><a href="/2021/ai/PageRank/" rel="bookmark" title="PageRank算法">PageRank算法</a></li><li><a href="/2021/ai/DGL-01-%E4%BF%A1%E6%81%AF%E4%BC%A0%E9%80%92%E8%8C%83%E5%BC%8F/" rel="bookmark" title="DGL(01): 信息传递范式">DGL(01): 信息传递范式</a></li><li><a href="/2021/ai/DGL-02-%E6%9E%84%E5%BB%BA%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" rel="bookmark" title="DGL(02): 构建图神经网络">DGL(02): 构建图神经网络</a></li><li><a href="/2021/ai/DGL-03-%E5%9B%BE%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E7%AE%A1%E9%81%93/" rel="bookmark" title="DGL(03): 图数据处理管道">DGL(03): 图数据处理管道</a></li><li><a href="/2021/ai/DGL-04-%E8%AE%AD%E7%BB%83%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" rel="bookmark" title="DGL(04): 训练图神经网络">DGL(04): 训练图神经网络</a></li><li class="active"><a href="/2021/GNN-02-HAN-Heterogeneous-Graph-Attention-Network-%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/" rel="bookmark" title="GNN(02): HAN(Heterogeneous Graph Attention Network)代码复现">GNN(02): HAN(Heterogeneous Graph Attention Network)代码复现</a></li></ul></div><div class="overview panel" data-title="站点概览"><div class="author" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="image" itemprop="image" alt="Kalice" data-src="/images/avatar.jpg"><p class="name" itemprop="name">Kalice</p><div class="description" itemprop="description">Life is not about lifestyle, it means Lithium and Ferrum.</div></div><nav class="state"><div class="item posts"><a href="/archives/"><span class="count">16</span> <span class="name">文章</span></a></div><div class="item categories"><a href="/categories/"><span class="count">6</span> <span class="name">分类</span></a></div><div class="item tags"><a href="/tags/"><span class="count">28</span> <span class="name">标签</span></a></div></nav><div class="social"></div><ul class="menu"><li class="item"><a href="/" rel="section"><i class="ic i-home"></i>首页</a></li><li class="item"><a href="/about/" rel="section"><i class="ic i-user"></i>关于</a></li><li class="item dropdown"><a href="javascript:void(0);"><i class="ic i-feather"></i>文章</a><ul class="submenu"><li class="item"><a href="/archives/" rel="section"><i class="ic i-list-alt"></i>归档</a></li><li class="item"><a href="/categories/" rel="section"><i class="ic i-th"></i>分类</a></li><li class="item"><a href="/tags/" rel="section"><i class="ic i-tags"></i>标签</a></li></ul></li></ul></div></div></div><ul id="quick"><li class="prev pjax"></li><li class="up"><i class="ic i-arrow-up"></i></li><li class="down"><i class="ic i-arrow-down"></i></li><li class="next pjax"></li><li class="percent"></li></ul></div></div><div class="dimmer"></div></div></main><footer id="footer"><div class="inner"><div class="widgets"><div class="rpost pjax"><h2>随机文章</h2><ul><li class="item"><div class="breadcrumb"><a href="/categories/ai/" title="分类于 AI">AI</a></div><span><a href="/2021/ai/DGL-01-%E4%BF%A1%E6%81%AF%E4%BC%A0%E9%80%92%E8%8C%83%E5%BC%8F/" title="DGL(01): 信息传递范式">DGL(01): 信息传递范式</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/python/" title="分类于 Python">Python</a></div><span><a href="/2021/python/pip%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/" title="pip 相关问题">pip 相关问题</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ai/" title="分类于 AI">AI</a></div><span><a href="/2021/GNN-02-HAN-Heterogeneous-Graph-Attention-Network-%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/" title="GNN(02): HAN(Heterogeneous Graph Attention Network)代码复现">GNN(02): HAN(Heterogeneous Graph Attention Network)代码复现</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ss/" title="分类于 琐事">琐事</a></div><span><a href="/2021/ss/zsh%E7%BE%8E%E5%8C%96/" title="zsh美化">zsh美化</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ai/" title="分类于 AI">AI</a></div><span><a href="/2021/ai/DGL-04-%E8%AE%AD%E7%BB%83%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" title="DGL(04): 训练图神经网络">DGL(04): 训练图神经网络</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/python/" title="分类于 Python">Python</a></div><span><a href="/2021/python/python%E4%B8%8D%E5%90%8C%E7%BA%A7%E6%A8%A1%E5%9D%97%E5%AF%BC%E5%85%A5/" title="python不同级模块导入">python不同级模块导入</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/js/" title="分类于 记事">记事</a></div><span><a href="/2021/js/%E4%BB%A3%E5%8A%9E/" title="待办">待办</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ai/" title="分类于 AI">AI</a></div><span><a href="/2021/ai/DGL-03-%E5%9B%BE%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E7%AE%A1%E9%81%93/" title="DGL(03): 图数据处理管道">DGL(03): 图数据处理管道</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/blog/" title="分类于 blog">blog</a></div><span><a href="/2021/blog/Hexo-Shoka%E4%B8%BB%E9%A2%98-git%E6%89%98%E7%AE%A1%EF%BC%8C%E6%8C%81%E7%BB%AD%E4%BC%98%E5%8C%96ing/" title="Hexo+Shoka主题+git托管，持续优化中">Hexo+Shoka主题+git托管，持续优化中</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ai/" title="分类于 AI">AI</a></div><span><a href="/2021/ai/DGL-02-%E6%9E%84%E5%BB%BA%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" title="DGL(02): 构建图神经网络">DGL(02): 构建图神经网络</a></span></li></ul></div><div><h2>最新评论</h2><ul class="leancloud-recent-comment"></ul></div></div><div class="status"><div class="copyright">&copy; 2010 – <span itemprop="copyrightYear">2021</span> <span class="with-love"><i class="ic i-sakura rotate"></i> </span><span class="author" itemprop="copyrightHolder">Kalice @ Songlin</span></div><div class="count"><span class="post-meta-item-icon"><i class="ic i-chart-area"></i> </span><span title="站点总字数">38k 字</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="ic i-coffee"></i> </span><span title="站点阅读时长">34 分钟</span></div><div class="powered-by">基于 <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & Theme.<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FtZWhpbWUvaGV4by10aGVtZS1zaG9rYQ==">Shoka</span></div></div></div></footer></div><script data-config type="text/javascript">var LOCAL={path:"2021/GNN-02-HAN-Heterogeneous-Graph-Attention-Network-代码复现/",favicon:{show:"（●´3｀●）やれやれだぜ",hide:"(´Д｀)大変だ！"},search:{placeholder:"文章搜索",empty:"关于 「 ${query} 」，什么也没搜到",stats:"${time} ms 内找到 ${hits} 条结果"},valine:!0,copy_tex:!0,katex:!0,mermaid:!0,fancybox:!0,copyright:'复制成功，转载请遵守 <i class="ic i-creative-commons"></i>BY-NC-SA 协议。',ignores:[function(e){return e.includes("#")},function(e){return new RegExp(LOCAL.path+"$").test(e)}]}</script><script src="https://cdn.polyfill.io/v2/polyfill.js"></script><script src="//cdn.jsdelivr.net/combine/npm/pace-js@1.0.2/pace.min.js,npm/pjax@0.2.8/pjax.min.js,npm/whatwg-fetch@3.4.0/dist/fetch.umd.min.js,npm/animejs@3.2.0/lib/anime.min.js,npm/algoliasearch@4/dist/algoliasearch-lite.umd.js,npm/instantsearch.js@4/dist/instantsearch.production.min.js,npm/lozad@1/dist/lozad.min.js,npm/quicklink@2/dist/quicklink.umd.js"></script><script src="/js/app.js?v=0.2.5"></script></body></html><!-- rebuild by hrmmi -->